# Суть проета  

Я попросил популярную нейросеть Deepseek придумать мне pet-проект. Я хотел дообучить какую-нибудь llm с помощью LoRA и использовать ее в качестве AI ассистента в виде Телеграм бота. Deepseek должен был предложить llm, которую я буду обучать, чему я ее буду обучать, подобрать датасет для обучения, а также помочь с настройкой параметров и фиксом мелких ошибок. Я должен был направлять его, фильтровать его решения, писать основной код и исправлять основные ошибки. Таким образом, я использовал Deepseek не как программиста, а как генератора идей.

После дискусси была выбрана модель Mistral 7B (https://huggingface.co/mistralai/Mistral-7B-v0.1).  
Целью было научить модель общаться на русском языке.  
Для обучения был выбран датасет ru_turbo_alpaca (https://huggingface.co/datasets/IlyaGusev/ru_turbo_alpaca).  

# Структура проекта  
RU_MISTRAL_7B_TG_BOT/  
├── final_model/                # Финальная дообученная модель  
├── model_training  
│   ├── checking_dataset.ipynb  
│   ├── lora_training.py  
│   ├── model_inference.py  
│   └── model_test_before_lora.py  
├── ru_turbo_alpaca_filtered/  # Датасет для обучения  
├── telegram_bot/  
│   └── bot.py                 # Телеграм-бот с интерфейсом к модели  
├── before_lora.txt            # Примеры ответов до обучения  
└── after_lora.txt             # Примеры ответов после обучения  
